{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":9330790,"sourceType":"datasetVersion","datasetId":5652965}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\nimport h5py\nimport os\nimport json\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers import ViTForImageClassification, ViTImageProcessor\nfrom accelerate import Accelerator\nimport os\nimport torch\nfrom transformers import (ViTForImageClassification, ViTImageProcessor, DeiTForImageClassification, \n                          CvtForImageClassification, AutoFeatureExtractor, CLIPModel, CLIPProcessor, \n                          ViTMAEForPreTraining, AutoImageProcessor, ViTModel)\nimport timm\nimport json\nimport pandas as pd \nfrom PIL import Image\nimport io\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-06T06:31:24.162055Z","iopub.execute_input":"2024-09-06T06:31:24.162975Z","iopub.status.idle":"2024-09-06T06:31:24.170654Z","shell.execute_reply.started":"2024-09-06T06:31:24.162932Z","shell.execute_reply":"2024-09-06T06:31:24.169673Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"accelerator = Accelerator()  # Automatically detects multi-GPU and mixed precision setups\ndevice = accelerator.device  # Get the device from Accelerator\n\nh5_file = \"/kaggle/input/isic-2024-challenge/test-image.hdf5\"\nbase_dir = \"/kaggle/input/2019-finetuned-vits/\" \n\nvit_models = [\n    (\"google/vit-base-patch16-224\", 64),\n    (\"facebook/deit-base-distilled-patch16-224\", 64),\n    (\"microsoft/cvt-13\", 64),\n    (\"facebook/dino-vitb16\", 64),\n    (\"facebook/vit-mae-base\", 32)\n]\n\nprint(\"The device used:\", device)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-06T06:31:24.172476Z","iopub.execute_input":"2024-09-06T06:31:24.172803Z","iopub.status.idle":"2024-09-06T06:31:24.185816Z","shell.execute_reply.started":"2024-09-06T06:31:24.172770Z","shell.execute_reply":"2024-09-06T06:31:24.184780Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"The device used: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"\nclass H5ImageDataset(Dataset):\n    def __init__(self, h5_file, preprocessor):\n        self.h5_file = h5_file\n        self.preprocessor = preprocessor\n        self.h5_data = h5py.File(h5_file, 'r')\n        self.image_ids = list(self.h5_data.keys())  # Assuming keys are image IDs\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_data = self.h5_data[image_id][()]  # Extract the image as an array\n        \n        # Check if the image data is in bytes and decode it\n        if isinstance(image_data, np.bytes_):\n            image_data = np.frombuffer(image_data, dtype=np.uint8)\n            image_data = Image.open(io.BytesIO(image_data))\n        elif isinstance(image_data, np.ndarray):\n            image_data = Image.fromarray(image_data)\n\n        # Now pass the image to the preprocessor\n        inputs = self.preprocessor(images=image_data, return_tensors=\"pt\")\n        return inputs['pixel_values'].squeeze(0), image_id\n\n\n# def get_model_and_preprocessor(model_dir, num_labels=2, base_dir=\"/kaggle/input/2019-finetuned-vits/\"):\n#     # Modify the model_dir string for local path compatibility\n#     model_dir = model_dir.replace(\"/\", \"_\").replace(\"-\", \"_\")\n    \n#     # Define the paths for the model and preprocessor\n#     config_path = os.path.join(base_dir, f\"{model_dir}_config.json\")\n#     model_path = os.path.join(base_dir, f\"best_model_{model_dir}.pth\")\n#     preprocessor_config_path = os.path.join(base_dir, f\"{model_dir}_preprocessor_config.json\")\n    \n#     # Load the preprocessor config\n#     with open(preprocessor_config_path, 'r') as f:\n#         preprocessor_config = json.load(f)\n\n#     if \"vit\" in model_dir:\n#         model = ViTForImageClassification.from_pretrained(config_path, num_labels=num_labels)\n#         preprocessor = ViTImageProcessor.from_pretrained(preprocessor_config)\n#     elif \"dino\" in model_dir:\n#         model = ViTModel.from_pretrained(config_path)\n#         preprocessor = ViTImageProcessor.from_pretrained(preprocessor_config)\n#     elif \"deit\" in model_dir:\n#         model = DeiTForImageClassification.from_pretrained(config_path, num_labels=num_labels)\n#         preprocessor = AutoFeatureExtractor.from_pretrained(preprocessor_config)\n#     elif \"cvt\" in model_dir:\n#         model = CvtForImageClassification.from_pretrained(config_path, num_labels=num_labels)\n#         preprocessor = AutoFeatureExtractor.from_pretrained(preprocessor_config)\n#     elif \"timm\" in model_dir:\n#         model = timm.create_model(model_dir.split('/')[-1], pretrained=True, num_classes=num_labels)\n#         data_config = timm.data.resolve_model_data_config(model)\n#         preprocessor = timm.data.create_transform(**data_config)\n#     elif \"clip\" in model_dir:\n#         model = CLIPModel.from_pretrained(config_path)\n#         preprocessor = CLIPProcessor.from_pretrained(preprocessor_config)\n#     elif \"mae\" in model_dir:\n#         model = ViTMAEForPreTraining.from_pretrained(config_path)\n#         preprocessor = AutoImageProcessor.from_pretrained(preprocessor_config)\n#     else:\n#         raise ValueError(f\"Unsupported model: {model_dir}\")\n    \n#     # Load the model's weights from the .pth file\n#     model.load_state_dict(torch.load(model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n#     model.eval()  # Set the model to evaluation mode\n\n#     return model, preprocessor\ndef get_model_and_preprocessor(model_dir, num_labels=2, base_dir=\"/kaggle/input/2019-finetuned-vits/\"):\n    model_dir = model_dir.replace(\"/\", \"_\").replace(\"-\", \"_\")\n    # Define the path to the model folder (no need to access individual files inside it)\n    model_folder = os.path.join(base_dir, model_dir + '/')\n\n    # Detect if there's a custom `best_model.pth` or use the default Hugging Face model\n    model_path = os.path.join(model_folder, \"best_model.pth\")\n\n    if \"vit\" in model_dir:\n        model = ViTForImageClassification.from_pretrained(model_folder, num_labels=num_labels)\n        preprocessor = ViTImageProcessor.from_pretrained(model_folder)\n    elif \"dino\" in model_dir:\n        model = ViTModel.from_pretrained(model_folder)\n        preprocessor = ViTImageProcessor.from_pretrained(model_folder)\n    elif \"deit\" in model_dir:\n        model = DeiTForImageClassification.from_pretrained(model_folder, num_labels=num_labels)\n        preprocessor = AutoFeatureExtractor.from_pretrained(model_folder)\n    elif \"cvt\" in model_dir:\n        model = CvtForImageClassification.from_pretrained(model_folder, num_labels=num_labels)\n        preprocessor = AutoFeatureExtractor.from_pretrained(model_folder)\n    elif \"timm\" in model_dir:\n        model = timm.create_model(model_dir.split('/')[-1], pretrained=True, num_classes=num_labels)\n        data_config = timm.data.resolve_model_data_config(model)\n        preprocessor = timm.data.create_transform(**data_config)\n    elif \"clip\" in model_dir:\n        model = CLIPModel.from_pretrained(model_folder)\n        preprocessor = CLIPProcessor.from_pretrained(model_folder)\n    elif \"mae\" in model_dir:\n        model = ViTMAEForPreTraining.from_pretrained(model_folder)\n        preprocessor = AutoImageProcessor.from_pretrained(model_folder)\n    else:\n        raise ValueError(f\"Unsupported model: {model_dir}\")\n\n    # Check if custom weights exist (best_model.pth)\n    if os.path.exists(model_path):\n        model.load_state_dict(torch.load(model_path, map_location=accelerator.device))\n    \n    model.eval()  # Set the model to evaluation mode\n\n    return model, preprocessor\n\ndef generate_predictions(model, preprocessor, h5_file, batch_size=32):\n    dataset = H5ImageDataset(h5_file, preprocessor)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    # Prepare the model and dataloader using Accelerator\n    model, dataloader = accelerator.prepare(model, dataloader)\n\n    predictions = {}\n    model.to(accelerator.device)\n\n    with torch.no_grad():\n        for batch, image_ids in tqdm(dataloader):\n            batch = batch.to(accelerator.device)\n            outputs = model(batch)\n            logits = outputs.logits\n            \n            # Apply softmax to get probabilities\n            probs = torch.softmax(logits, dim=-1)\n\n            # Extract the probability of class 1 (positive class)\n            class_1_probs = probs[:, 1].cpu().numpy()  # Get probability for class 1\n            \n            # Collect results\n            for image_id, prob in zip(image_ids, class_1_probs):\n                predictions[image_id] = prob  # Store only class 1 probability\n\n    return predictions\n\ndef average_predictions(model_preds_list):\n    \"\"\"\n    Average predictions from multiple models.\n    \"\"\"\n    combined_predictions = {}\n    num_models = len(model_preds_list)\n\n    for model_preds in model_preds_list:\n        for image_id, pred in model_preds.items():\n            if image_id not in combined_predictions:\n                combined_predictions[image_id] = np.zeros_like(pred)\n            combined_predictions[image_id] += pred / num_models  # Average probabilities\n\n    return combined_predictions\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-06T06:31:24.187133Z","iopub.execute_input":"2024-09-06T06:31:24.187436Z","iopub.status.idle":"2024-09-06T06:31:24.212140Z","shell.execute_reply.started":"2024-09-06T06:31:24.187405Z","shell.execute_reply":"2024-09-06T06:31:24.211355Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"\nall_model_predictions = []\nfor model_dir, batch_size in vit_models:\n    model, preprocessor = get_model_and_preprocessor(model_dir)\n    predictions = generate_predictions(model, preprocessor, h5_file, batch_size=batch_size)\n    all_model_predictions.append(predictions)\n\n# Average predictions if using multiple models\nif len(all_model_predictions) > 1:\n    averaged_predictions = average_predictions(all_model_predictions)\nelse:\n    averaged_predictions = all_model_predictions[0]","metadata":{"execution":{"iopub.status.busy":"2024-09-06T06:31:24.213375Z","iopub.execute_input":"2024-09-06T06:31:24.213739Z","iopub.status.idle":"2024-09-06T06:31:26.519345Z","shell.execute_reply.started":"2024-09-06T06:31:24.213697Z","shell.execute_reply":"2024-09-06T06:31:26.518351Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 15.52it/s]\n100%|██████████| 1/1 [00:00<00:00, 15.33it/s]\n100%|██████████| 1/1 [00:00<00:00, 20.33it/s]\n100%|██████████| 1/1 [00:00<00:00, 15.77it/s]\n100%|██████████| 1/1 [00:00<00:00, 15.82it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"df_sub = pd.read_csv(\"/kaggle/input/isic-2024-challenge/sample_submission.csv\")\ndf_sub[\"target\"] = df_sub[\"isic_id\"].map(averaged_predictions)\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub","metadata":{"execution":{"iopub.status.busy":"2024-09-06T06:31:26.521235Z","iopub.execute_input":"2024-09-06T06:31:26.521564Z","iopub.status.idle":"2024-09-06T06:31:26.535997Z","shell.execute_reply.started":"2024-09-06T06:31:26.521529Z","shell.execute_reply":"2024-09-06T06:31:26.535025Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"        isic_id        target\n0  ISIC_0015657  0.0053302096\n1  ISIC_0015729  0.0035101308\n2  ISIC_0015740   0.011597361","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>isic_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0015657</td>\n      <td>0.0053302096</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0015729</td>\n      <td>0.0035101308</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0015740</td>\n      <td>0.011597361</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}